K-Nearest Neighbor (KNN) Algorithm – 

Definition:
K-Nearest Neighbor (KNN) is a supervised machine learning algorithm used for both classification and regression problems.
It predicts the output for a new data point by looking at the ‘K’ nearest data points in the training dataset and choosing the most common class (for classification) or average value (for regression).


---

Working of KNN Algorithm:

1. Choose the number of neighbors (K):

Select a suitable value of K (e.g., 3, 5, 7).

K decides how many nearby points will be considered when making a prediction.



2. Calculate the distance:

Find the distance between the new data point and all other data points in the training set.

Common distance formulas used are:

Euclidean Distance





d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}

3. Find the nearest neighbors:

Sort all distances and select the K smallest distances (the closest points).



4. Voting or Averaging:

For classification: Choose the class that appears most among the K neighbors.

For regression: Take the average of the K nearest values.



5. Predict the result:

The selected class or averaged value becomes the model’s prediction.





---

Example:

Suppose we want to classify whether a fruit is an apple or orange based on its weight and color.
If K = 3, and among the 3 nearest neighbors,

2 are apples and 1 is orange → the new fruit is predicted as apple.



---

Advantages:

Simple and easy to implement.

No training phase required (lazy learner).

Works well with small datasets.



---

Disadvantages:

Slow for large datasets because it compares every point.

Sensitive to noisy data and irrelevant features.

Requires normalization of data for accurate distance calculation.



---

✅ In short:
KNN is a lazy learning algorithm that classifies or predicts based on similarity (distance) between data points.
It’s simple and effective for small datasets but less efficient for large or high-dimensional data.