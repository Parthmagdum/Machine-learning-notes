### Bias and Variance — Detailed Explanation (8 Marks)

**Definition (short):**  
- **Bias** is the error from **wrong assumptions** in the learning algorithm. High bias means the model is too simple and cannot capture the true pattern (underfitting).  
- **Variance** is the error from **too much sensitivity** to the training data. High variance means the model fits noise and does not generalize well (overfitting).

---

### Error decomposition (important concept)
For a prediction problem, the expected squared error at a point can be decomposed as:

Expected Error = Bias² + Variance + Irreducible Error (noise)

- **Bias²:** Error from the model’s assumptions (systematic error).  
- **Variance:** Error from model’s sensitivity to training data (how much predictions change with different training sets).  
- **Irreducible Error:** Noise in data we cannot remove.

---

### Intuition with examples
- **High Bias (Underfitting):**  
  - Example: Using a straight line (linear model) to fit strongly curved data.  
  - Effect: Both training and test errors are high. Model is too simple.

- **High Variance (Overfitting):**  
  - Example: A very deep decision tree that perfectly classifies training data but fails on new data.  
  - Effect: Training error is low, test error is high. Model learned noise.

---

### How to detect (practical signs)
- **Learning curves (train vs test error as data size grows):**
  - **High bias:** Both train & test errors high and close to each other; adding more data usually won’t help much.
  - **High variance:** Training error low, test error much higher; adding more training data often reduces variance and test error.
- **Cross-validation:** Large gap between cross-val performance and training performance indicates high variance.

---

### How to fix / reduce
- **To reduce Bias (make model more flexible):**
  - Use a more complex model (e.g., add polynomial features, deeper tree, neural network).  
  - Add relevant features (feature engineering).  
  - Reduce regularization strength.

- **To reduce Variance (make model simpler or more robust):**
  - Get more training data.  
  - Use simpler model or reduce model complexity.  
  - Apply regularization (L1, L2).  
  - Use ensemble methods (Bagging, Random Forest) or dropout (for neural nets).  
  - Feature selection / dimensionality reduction (PCA).

---

### Practical tradeoff & strategy
- There is no free lunch — decreasing bias often increases variance and vice versa.  
- **Goal:** find the sweet spot that minimizes total error (Bias² + Variance).  
- Typical workflow:
  1. Start simple, check errors.  
  2. If underfitting → increase complexity / add features.  
  3. If overfitting → regularize / get more data / simplify model.  
  4. Use cross-validation to choose hyperparameters that balance bias and variance.

---

### In short (one-liner)
- **Bias** = error from wrong assumptions (underfit).  
- **Variance** = error from sensitivity to data (overfit).  
Balance them to minimize total prediction error.