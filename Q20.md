### Difference Between Bagging and Aggregation (4 Marks)

| **Aspect** | **Bagging (Bootstrap Aggregating)** | **Aggregation** |
|-------------|-------------------------------------|------------------|
| **Definition** | Bagging is a Machine Learning ensemble technique that combines results from **multiple models trained on different bootstrap samples** of the data. | Aggregation is the **process of combining the outputs** of multiple models to produce a **final prediction**. |
| **Data Used** | Uses **bootstrap samples** (sampling with replacement) from the training data. | Uses **predictions from different models** — may or may not use bootstrap samples. |
| **Purpose** | To **reduce variance** and **prevent overfitting** in high-variance models like decision trees. | To **combine predictions** for improved overall accuracy and stability. |
| **Example** | Random Forest uses bagging to train many decision trees on different bootstrap samples. | The final voting or averaging step that combines all model outputs is aggregation. |

---

✅ **In short:**  
Bagging is the **method of training models** on bootstrap samples, while aggregation is the **step of combining their predictions** to form the final result.